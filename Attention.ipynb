{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T05:00:04.441864Z",
     "start_time": "2025-04-20T05:00:01.755616Z"
    }
   },
   "source": [
    "import torch\n",
    "from jax.experimental.pallas.ops.gpu.attention_mgpu import attention"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:34.891084Z",
     "start_time": "2025-04-18T05:50:34.855451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ],
   "id": "cfb7e84ea625ff81",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:36.209428Z",
     "start_time": "2025-04-18T05:50:36.179286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_query = inputs[1]\n",
    "input_query"
   ],
   "id": "8e2ee7f9b595be11",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5500, 0.8700, 0.6600])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:37.500048Z",
     "start_time": "2025-04-18T05:50:37.487090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_1 = inputs[0]\n",
    "input_1"
   ],
   "id": "f6d17690cce53ca6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4300, 0.1500, 0.8900])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:38.768813Z",
     "start_time": "2025-04-18T05:50:38.751810Z"
    }
   },
   "cell_type": "code",
   "source": "torch.dot(input_query,input_1)",
   "id": "bd828829213a61f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9544)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:40.220946Z",
     "start_time": "2025-04-18T05:50:40.205059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "atn_score = torch.empty(inputs.shape[0])\n",
    "for id,value in enumerate(inputs):\n",
    "    atn_score[id] = torch.dot(value,input_query)\n",
    "\n",
    "atn_score"
   ],
   "id": "c7c7cceb4a1b9c3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "q@k^T computes the dot product between query and keys, giving you attention scores\n",
    "softmax(q@k^T) converts these scores into a probability distribution (attention weights)\n",
    "softmax(q@k^T)V multiplies these attention weights with the value vectors and sums them up"
   ],
   "id": "8e060c8db824ed0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What is happening is we are trying to find what is the most important token values for given\n",
    "token for all tokens. i say when you dont understand it again, you should draw the some pictures and\n",
    "make what is dot products gives."
   ],
   "id": "f04dc1790b2ed689"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:42.001101Z",
     "start_time": "2025-04-18T05:50:41.991940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ],
   "id": "9f81c1ffb9a1b57d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:43.652380Z",
     "start_time": "2025-04-18T05:50:43.623848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ],
   "id": "318cd0d9f95387cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:45.049627Z",
     "start_time": "2025-04-18T05:50:45.028338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ],
   "id": "357e727dd2503dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[batch,seq,feature] -----> dim = 0 : batch ,dim = 1 : seq ,dim = 2 : features",
   "id": "a0ba0817f6dcab9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:46.572603Z",
     "start_time": "2025-04-18T05:50:46.563882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_context = attn_weights @ inputs\n",
    "print(all_context)"
   ],
   "id": "12466dac916f4625",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T05:00:20.327222Z",
     "start_time": "2025-04-20T05:00:20.322036Z"
    }
   },
   "cell_type": "code",
   "source": "import torch.nn as nn",
   "id": "1327f6a3527e3d09",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:50.732501Z",
     "start_time": "2025-04-18T05:50:50.721437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,embed_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = embed_dimension\n",
    "\n",
    "        self.Q = nn.Linear(embed_dimension,embed_dimension)\n",
    "        self.K = nn.Linear(embed_dimension,embed_dimension)\n",
    "        self.V = nn.Linear(embed_dimension,embed_dimension)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        batch_size, seq_len, embed_dim = inputs.shape\n",
    "        queries = self.Q(inputs)\n",
    "        keys = self.K(inputs)\n",
    "        values = self.V(inputs)\n",
    "        keys_transposed = keys.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.bmm(queries, keys_transposed) / torch.sqrt(torch.tensor(self.embed, dtype=torch.float32))\n",
    "\n",
    "        #nn.Softmax is module like nn.MSELoss\n",
    "        attention_weights = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        context_vector = torch.bmm(attention_weights, values)\n",
    "        return context_vector\n"
   ],
   "id": "e477df0b4b288933",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:53.528282Z",
     "start_time": "2025-04-18T05:50:53.429174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inp = torch.randn(8,4,256)\n",
    "atn = Attention(256)\n",
    "score = atn(inp)"
   ],
   "id": "74e1458a7687b8dd",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:57.141422Z",
     "start_time": "2025-04-18T05:50:57.131997Z"
    }
   },
   "cell_type": "code",
   "source": "score.shape",
   "id": "9d6072718e28fdf1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:50:58.501066Z",
     "start_time": "2025-04-18T05:50:58.482425Z"
    }
   },
   "cell_type": "code",
   "source": "score[0][0]",
   "id": "b3cfff12d891661b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4144,  0.3002, -0.0115, -0.1468,  0.2303, -0.1717, -0.0537,  0.1635,\n",
       "        -0.3045, -0.1517, -0.1498,  0.0202, -0.3873,  0.1422, -0.2053, -0.3109,\n",
       "         0.0830,  0.2286, -0.1070,  0.3913, -0.0706,  0.5189, -0.1951,  0.3627,\n",
       "        -0.2387,  0.1900, -0.2856,  0.0265, -0.0230,  0.4463,  0.1618, -0.2574,\n",
       "        -0.2262, -0.4509, -0.0500,  0.2093, -0.1159,  0.3719, -0.1064, -0.1726,\n",
       "        -0.0714, -0.2970,  0.0770,  0.2337,  0.4028, -0.3590,  0.3574,  0.6720,\n",
       "         0.5702, -0.0287,  0.2062,  0.4490,  0.1704,  0.3056,  0.0718, -0.1337,\n",
       "         0.1799, -0.2567,  0.6767, -0.4014,  0.2725, -0.1527,  0.3658, -0.1617,\n",
       "         0.3129,  0.3069,  0.3944,  0.2447, -0.0680,  0.4160, -0.6410, -0.1520,\n",
       "         0.0715, -0.1772,  0.4450,  0.0840, -0.2896, -0.3560,  0.4340,  0.0432,\n",
       "         0.0194, -0.3525,  0.1243,  0.2352,  0.3248, -0.1320,  0.2128,  0.2866,\n",
       "        -0.1223,  0.2483, -0.7347,  0.2327,  0.0905,  0.6703, -0.1278, -0.0797,\n",
       "         0.3985, -0.5234, -0.0574, -0.5705,  0.2540,  0.0080,  0.5227, -0.0687,\n",
       "         0.4490, -0.1828, -0.7694,  0.1605,  0.5498, -0.5200,  0.2092,  0.6422,\n",
       "        -0.1853, -0.0319, -0.5203, -0.0100, -0.3524,  0.2142,  0.2417,  0.2642,\n",
       "        -0.2172,  0.0595, -0.1762, -0.1533, -0.7769,  0.3739,  0.1984,  0.5919,\n",
       "        -0.4545,  0.0686,  0.4347, -0.4068,  0.2024,  0.3505, -0.0056, -0.0723,\n",
       "        -0.6265, -1.1929, -0.0781, -0.0927, -0.3260, -0.4844, -0.0015, -0.0973,\n",
       "         0.4410,  0.1113,  0.3675,  0.2716, -0.1931, -0.0882,  0.2720,  0.1105,\n",
       "         0.3859,  0.3860, -0.2863, -0.2715,  0.1240, -0.2881,  0.0332, -0.4471,\n",
       "         0.2021,  0.3057, -0.0203,  0.2644, -0.2461,  0.3172, -0.6212,  0.1884,\n",
       "        -0.1197, -0.2883,  0.1109,  0.0650, -0.7739, -0.1588, -0.1864,  0.0705,\n",
       "         0.4895, -0.3219, -0.0476, -0.1281, -0.3706, -0.3477, -0.0408,  0.2434,\n",
       "        -0.2133, -0.0972, -0.4733, -0.5409, -0.1380, -0.2879,  0.2146,  0.1982,\n",
       "         0.3042,  0.0508,  0.6066,  0.0554, -0.2210, -0.2734, -0.1190,  0.7291,\n",
       "        -0.0352, -0.0380,  0.0360, -0.2414,  0.1228,  0.0612, -0.0591,  0.1855,\n",
       "        -0.3966, -0.2420,  0.1825, -0.2034,  0.0925,  0.3660,  0.0681,  0.0168,\n",
       "        -0.2816, -0.3935,  0.0691,  0.3220,  0.1756, -0.2873,  0.1388,  0.2881,\n",
       "         0.1187,  0.6051,  0.3977,  0.2458, -0.4329,  0.0041,  0.1436, -0.2716,\n",
       "        -0.0769, -0.3070, -0.0027,  0.2137, -0.1387,  0.3200, -0.5319,  0.0693,\n",
       "         0.4474, -0.6260, -0.1494,  0.4300,  0.1734,  0.4397, -0.4713, -0.1851,\n",
       "        -0.2099, -0.1677, -0.3381, -0.2268, -0.0210, -0.1395,  0.0452, -0.5325],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How Language Models Are Actually Trained\n",
    "Language models like GPT are typically trained using a technique called \"autoregressive language modeling\" or \"next-token prediction.\" Here's how it works:\n",
    "\n",
    "We take a sequence like \"I'm going to school tomorrow\"\n",
    "We don't split it into separate \"input\" and \"output\" parts\n",
    "Instead, we train the model to predict each token based on all previous tokens\n",
    "\n",
    "So the training pairs look like:\n",
    "\n",
    "Given \"I'm\", predict \"going\"\n",
    "Given \"I'm going\", predict \"to\"\n",
    "Given \"I'm going to\", predict \"school\"\n",
    "Given \"I'm going to school\", predict \"tomorrow\"\n",
    "\n",
    "Each position in the sequence serves as both input (for later predictions) and target (for the prediction at that position).\n",
    "Why Causal Masking Is Essential\n",
    "Now you can see why causal masking is crucial:\n",
    "Without masking, when trying to predict \"school\", the model would have access to \"tomorrow\" in its attention mechanism. This defeats the purpose of predicting the next word, since the model already sees it!\n",
    "The causal mask ensures that prediction at each position can only use information from previous positions, matching how the model will be used during generation.\n",
    "Contrast with Traditional Supervised Learning\n",
    "The approach you initially described:\n",
    "\n",
    "\"Data is 'I'm going to school', label is 'going to school tomorrow'\"\n",
    "\n",
    "This is more like a traditional encoder-decoder setup (e.g., for translation or summarization) where you have distinct input and output sequences. While some language models are trained this way for specific tasks, the fundamental pretraining of models like GPT uses the autoregressive approach I described above."
   ],
   "id": "7dd796ff5fce37a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:51:01.731628Z",
     "start_time": "2025-04-18T05:51:01.719011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionMask(nn.Module):\n",
    "    def __init__(self,embed_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed = embed_dimension\n",
    "\n",
    "        self.Q = nn.Linear(embed_dimension,embed_dimension)\n",
    "        self.K = nn.Linear(embed_dimension,embed_dimension)\n",
    "        self.V = nn.Linear(embed_dimension,embed_dimension)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        batch_size, seq_len, embed_dim = inputs.shape\n",
    "        queries = self.Q(inputs)\n",
    "        keys = self.K(inputs)\n",
    "        values = self.V(inputs)\n",
    "        keys_transposed = keys.transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.bmm(queries, keys_transposed) / torch.sqrt(torch.tensor(self.embed, dtype=torch.float32))\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).to(inputs.device)\n",
    "        masked_attention_scores = attention_scores.masked_fill_(mask == 0, float('-inf'))\n",
    "        attention_weights = nn.Softmax(dim=-1)(masked_attention_scores)\n",
    "\n",
    "        context_vector = torch.bmm(attention_weights, values)\n",
    "        return context_vector\n"
   ],
   "id": "9ca739ad952f1c8c",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:51:03.796464Z",
     "start_time": "2025-04-18T05:51:03.745919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inp = torch.randn(8,4,256)\n",
    "atn = AttentionMask(256)\n",
    "score = atn(inp)"
   ],
   "id": "d0a9e2e7bd05e1f1",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:51:16.094631Z",
     "start_time": "2025-04-18T05:51:16.081986Z"
    }
   },
   "cell_type": "code",
   "source": "score.shape",
   "id": "644942a89344d0b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-18T05:51:27.246025Z",
     "start_time": "2025-04-18T05:51:27.227472Z"
    }
   },
   "cell_type": "code",
   "source": "score[0][0]",
   "id": "2efb7cb60f615afd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.8060e-01,  3.0595e-01, -2.3299e-01,  8.4332e-01, -1.5796e+00,\n",
       "         4.7608e-01,  9.8869e-01,  9.1234e-01,  3.2835e-01, -1.0854e-01,\n",
       "        -2.5397e-01, -9.7945e-01, -4.6993e-01,  5.0027e-01, -1.4757e-01,\n",
       "         5.7748e-01,  6.7749e-01, -4.5327e-01,  9.3564e-01,  6.2664e-01,\n",
       "        -1.1262e+00, -4.3796e-02, -4.2575e-01,  1.8760e-01,  7.8315e-01,\n",
       "         5.7906e-01, -8.3422e-01, -7.9052e-01,  2.2270e-01,  4.6146e-02,\n",
       "        -1.1209e+00, -3.9621e-01,  3.6719e-02,  7.5170e-01, -7.0961e-01,\n",
       "        -4.9098e-01,  1.2279e-01, -2.5331e-01,  3.3222e-01,  2.4698e-01,\n",
       "         1.1430e+00,  7.5232e-03, -2.7155e-01, -9.2286e-01, -2.1283e-01,\n",
       "        -1.3296e-01, -1.3904e-01, -2.1041e-01, -1.5030e-01,  3.0007e-01,\n",
       "        -2.8098e-01, -4.1020e-01, -6.5312e-02, -5.9277e-01, -8.7140e-01,\n",
       "        -1.4851e+00,  6.8901e-01,  6.2157e-02, -2.5424e-01,  9.6720e-01,\n",
       "         1.5572e-01, -7.8172e-01, -1.0494e-01, -4.3082e-01,  7.9993e-01,\n",
       "        -8.9199e-02,  2.2814e-01,  7.5920e-02, -6.6372e-01, -7.6957e-01,\n",
       "         4.7548e-01, -4.8137e-01, -3.8295e-01,  4.9549e-01,  3.1429e-01,\n",
       "        -6.4076e-01,  2.5289e-01,  5.9831e-01, -8.8061e-02,  7.9254e-01,\n",
       "         5.9967e-02, -1.7095e-01, -7.7228e-02, -4.6529e-02, -4.1189e-01,\n",
       "        -1.5875e-01,  9.4385e-01, -1.4061e+00, -3.9305e-01, -5.5166e-01,\n",
       "        -9.7208e-01,  5.1214e-02, -3.7948e-01,  3.0298e-01,  1.1124e-03,\n",
       "         7.1894e-01,  2.0825e-01, -1.1236e-01,  3.9118e-01,  7.0373e-01,\n",
       "        -5.9222e-01, -1.1036e+00,  1.1562e-01,  3.7736e-01, -2.4940e-01,\n",
       "        -7.2092e-01,  2.5472e-01,  4.4529e-01,  2.7021e-01,  1.0230e+00,\n",
       "        -7.3337e-01,  5.5643e-01, -1.5971e-01,  8.6970e-01,  6.9433e-01,\n",
       "         7.5748e-02, -5.9570e-01, -1.0042e-01,  3.9059e-01, -8.3801e-01,\n",
       "        -4.7560e-01, -2.6049e-01,  3.3534e-01, -1.1942e-01,  6.6887e-02,\n",
       "        -5.7069e-02, -7.3282e-01,  5.9242e-01,  2.2650e-01, -2.4456e-01,\n",
       "        -6.7456e-02,  1.1962e-01,  4.2807e-02,  3.3870e-01, -3.0573e-01,\n",
       "         2.8343e-01,  5.5421e-01, -2.9045e-01, -2.5580e-01,  5.4209e-01,\n",
       "        -2.8696e-01,  8.4628e-01, -1.6046e+00,  3.4888e-01, -9.7762e-01,\n",
       "         7.1296e-01,  1.4620e-01,  5.4965e-01, -1.5719e-01, -4.0727e-01,\n",
       "         6.3691e-02, -7.0875e-01, -4.6897e-01, -8.9942e-01, -1.5622e-02,\n",
       "         1.2554e-01, -1.6214e+00, -9.8513e-01, -5.6821e-01,  9.0528e-01,\n",
       "        -8.9162e-01, -1.8046e-01, -9.8645e-02,  2.0449e-02, -4.6219e-03,\n",
       "        -1.9892e-01, -5.9567e-01,  5.6362e-02, -3.5364e-01,  3.3057e-01,\n",
       "        -2.5295e-01,  1.9415e-02,  1.1768e+00,  2.5189e-01,  6.0547e-01,\n",
       "         4.2236e-01, -6.1119e-01,  3.8918e-01, -2.8689e-01,  8.5014e-01,\n",
       "        -1.5192e-01, -2.3372e-02,  1.3176e+00,  4.7011e-01, -4.9906e-01,\n",
       "        -2.1854e-01, -5.5793e-01,  4.0254e-02,  7.6550e-01,  1.0333e+00,\n",
       "        -5.8838e-01,  3.0586e-01, -1.3312e+00,  1.4000e-01, -7.8521e-01,\n",
       "         1.8011e-01, -4.8220e-01, -2.7947e-01,  1.0676e+00, -4.3241e-01,\n",
       "        -1.1351e-01, -2.4641e-01,  2.9648e-01,  4.6590e-01,  8.5956e-01,\n",
       "        -1.2764e-01, -6.2610e-01,  8.7146e-01,  1.8005e+00,  3.8068e-01,\n",
       "         1.3958e-01, -6.1211e-01, -8.2121e-01,  2.7614e-01,  2.4011e-01,\n",
       "         5.2205e-02, -4.1893e-01,  2.8725e-02,  4.5068e-01, -2.7072e-01,\n",
       "        -6.3558e-01, -3.0020e-01, -9.3794e-01,  2.8491e-01, -1.6500e-01,\n",
       "        -4.4302e-01,  1.2465e+00, -1.2320e+00,  1.7264e-01, -2.4520e-01,\n",
       "        -3.3802e-02, -1.2132e+00, -1.0335e-01,  2.0177e-02, -4.7221e-01,\n",
       "         2.5310e-04, -3.6267e-01,  8.2683e-01, -5.2882e-01, -3.5126e-01,\n",
       "         1.1181e+00, -4.2016e-01,  3.9937e-01,  3.1376e-01,  6.1172e-01,\n",
       "         7.1560e-02, -1.8400e-01,  1.4956e-01,  1.6274e-01,  3.2520e-01,\n",
       "        -2.5053e-01, -7.1295e-01, -4.6403e-01, -3.9519e-01, -3.4042e-01,\n",
       "        -4.9815e-01], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T11:14:09.960727Z",
     "start_time": "2025-04-20T11:14:09.950855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#I previously didnt include context-length , because i created the mask on the forward pass,\n",
    "#by that way we dont need a created a mask on full length of th model accept. but those mask\n",
    "# have to create each forward pass it is slower when training.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embed_in,embed_out,context_length ,heads,dropout=0,bias = False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert embed_out % heads == 0, \"embed_out must be divisible by heads\"\n",
    "\n",
    "        self.heads = heads\n",
    "        self.d_size = embed_out // heads\n",
    "\n",
    "        self.Q = nn.Linear(embed_in,embed_out,bias=bias)\n",
    "        self.K = nn.Linear(embed_in,embed_out,bias=bias)\n",
    "        self.V = nn.Linear(embed_in,embed_out,bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(embed_out,embed_out)\n",
    "        #by giving in mask into register buffer stat_dict() save this as well, and\n",
    "        # easy when model.to(Device)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length),\n",
    "                                diagonal=1))\n",
    "\n",
    "    def forward(self,text):\n",
    "        batch,seq_len,_ = text.shape\n",
    "        Q_text = self.Q(text)\n",
    "        K_text = self.K(text)\n",
    "        V_text = self.V(text)\n",
    "\n",
    "        Q_text = Q_text.view(batch,seq_len,self.heads,self.d_size).transpose(1,2)\n",
    "        K_text = K_text.view(batch,seq_len,self.heads,self.d_size).transpose(1,2)\n",
    "        V_text = V_text.view(batch,seq_len,self.heads,self.d_size).transpose(1,2)\n",
    "\n",
    "        attention_score = Q_text @ K_text.transpose(-2, -1) # we cant use torch.bmm() in 4D\n",
    "        mask = self.mask.bool()[:seq_len,:seq_len]\n",
    "        attention_weights = torch.softmax(attention_score.masked_fill_(mask,-torch.inf)/K_text.shape[-1]**0.5,dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        context_vector = attention_weights @ V_text\n",
    "        context_vector = context_vector.reshape(batch,seq_len,-1)\n",
    "        return self.projection(context_vector)\n",
    "\n",
    "\n"
   ],
   "id": "970bb89fd06df1b3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-20T11:14:11.055719Z",
     "start_time": "2025-04-20T11:14:10.971863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "atn = MultiHeadAttention(256 ,256,5,heads=8)\n",
    "print(atn(torch.randn(8,4,256)))"
   ],
   "id": "d047d9ffe13372e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.2263e-02,  1.7006e-02, -6.6266e-01,  ..., -2.0844e-01,\n",
      "           2.0626e-01, -2.0393e-01],\n",
      "         [ 6.9717e-02, -5.2152e-01,  6.1783e-02,  ...,  1.1413e-01,\n",
      "          -1.4347e-01, -3.0959e-01],\n",
      "         [ 1.0778e-01,  2.0654e-01, -3.2684e-01,  ...,  1.2353e-01,\n",
      "          -8.2114e-02,  3.0593e-01],\n",
      "         [-1.9528e-01,  3.4111e-01, -3.2996e-01,  ..., -1.7057e-01,\n",
      "          -2.4177e-01,  9.9616e-02]],\n",
      "\n",
      "        [[-5.8231e-01,  3.3775e-02,  2.7034e-02,  ..., -2.7374e-01,\n",
      "           4.8551e-02,  2.4596e-01],\n",
      "         [-3.2613e-01, -2.0954e-01, -3.9494e-02,  ..., -1.4908e-01,\n",
      "          -2.7056e-01, -1.9840e-02],\n",
      "         [ 4.8427e-02, -1.8274e-01,  4.8473e-01,  ...,  2.3740e-01,\n",
      "           5.4003e-02, -1.1858e-01],\n",
      "         [ 1.1849e-01, -3.5461e-01, -2.8840e-01,  ...,  6.3750e-01,\n",
      "           2.5891e-01,  2.1848e-01]],\n",
      "\n",
      "        [[-2.9731e-01, -1.4747e-01, -2.1943e-01,  ...,  7.4954e-02,\n",
      "          -3.1236e-02,  5.8017e-02],\n",
      "         [-2.2283e-02, -9.6130e-03,  3.8290e-01,  ..., -9.0732e-02,\n",
      "          -1.9343e-01, -3.3510e-01],\n",
      "         [-5.2704e-01, -4.5979e-02, -1.5266e-01,  ..., -3.4470e-01,\n",
      "           2.5051e-01,  3.6435e-02],\n",
      "         [ 2.8290e-01,  1.8109e-01, -5.4753e-02,  ..., -1.0778e-01,\n",
      "           4.7908e-01, -6.3866e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.8884e-01,  1.5526e-01,  2.7919e-01,  ..., -6.0571e-01,\n",
      "           1.6950e-01,  8.0508e-01],\n",
      "         [-1.7934e-01, -1.7512e-01, -1.5232e-01,  ..., -2.3544e-02,\n",
      "          -2.6409e-02, -2.9266e-02],\n",
      "         [-6.2055e-02, -1.4793e-01,  2.9006e-01,  ..., -5.5266e-01,\n",
      "          -1.9940e-01, -8.7819e-02],\n",
      "         [ 9.9534e-02, -1.6473e-01,  2.1616e-01,  ..., -5.7395e-01,\n",
      "          -3.6022e-01,  8.5044e-02]],\n",
      "\n",
      "        [[-1.1294e-01, -5.7282e-04, -9.0190e-02,  ..., -1.9960e-01,\n",
      "           1.0123e-01, -2.8731e-01],\n",
      "         [ 2.2565e-03, -5.5962e-01,  1.1123e-01,  ..., -5.6761e-02,\n",
      "          -1.1988e-01,  2.0232e-01],\n",
      "         [-3.3773e-01,  2.7623e-01, -1.0216e-02,  ..., -3.7135e-01,\n",
      "           6.0418e-02, -7.1574e-02],\n",
      "         [-1.6528e-01, -2.1790e-01, -1.9619e-01,  ...,  4.4205e-01,\n",
      "           4.3148e-01, -4.7351e-02]],\n",
      "\n",
      "        [[-4.2270e-01,  2.1905e-01, -1.0106e-01,  ...,  1.9170e-01,\n",
      "           2.9149e-01,  1.3563e-01],\n",
      "         [-1.4942e-01,  3.5594e-01,  6.8655e-02,  ...,  1.7361e-01,\n",
      "           2.4362e-01, -1.4090e-01],\n",
      "         [ 4.6297e-02,  1.3666e-01, -4.6568e-02,  ...,  1.8760e-01,\n",
      "          -1.6063e-01, -2.5293e-01],\n",
      "         [ 4.6476e-02,  8.2108e-02,  1.2293e-01,  ..., -2.7871e-01,\n",
      "          -5.9914e-02,  1.1175e-02]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "85b410c9705e2905"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
