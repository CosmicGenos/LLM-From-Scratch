{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Token-by-Token Generation in LLMs\n",
    "\n",
    "When you input \"how about\" to an LLM during inference, here's what actually happens:\n",
    "\n",
    "1. First, your input is tokenized. Depending on the tokenizer, \"how about\" might be split into separate tokens like [\"how\", \"about\"] or possibly even [\"how\", \" about\"] (note the space).\n",
    "\n",
    "2. The model processes these tokens sequentially to generate the next token:\n",
    "   - It first processes \"how\"\n",
    "   - Then considers both \"how\" and \"about\" together\n",
    "\n",
    "3. The model doesn't generate output for \"how\" and then separately for \"about\". Instead, it uses the full context [\"how\", \"about\"] to predict the next token.\n",
    "\n",
    "## The Output Mechanism\n",
    "\n",
    "For each prediction step, the final layer of the LLM maps from the embedding space to vocabulary space:\n",
    "\n",
    "- The output shape is indeed [batch, seq, vocabulary_size]\n",
    "- For the last position in the sequence, the model applies softmax to get a probability distribution over the entire vocabulary\n",
    "- The token with the highest probability (or sampled according to some strategy like temperature sampling) becomes the next token in the sequence\n",
    "\n",
    "## Concrete Example\n",
    "\n",
    "Let's trace through what happens when you input \"how about\":\n",
    "\n",
    "1. Input: \"how about\"\n",
    "2. Tokenized: [\"how\", \"about\"]\n",
    "3. Model processes both tokens\n",
    "4. For position after \"about\", the model outputs a probability distribution over the entire vocabulary\n",
    "5. Let's say \"you\" has the highest probability\n",
    "6. \"you\" is generated and added to the context\n",
    "7. New context: [\"how\", \"about\", \"you\"]\n",
    "8. Model then uses this full context to predict the next token\n",
    "\n",
    "The key insight is that LLMs don't generate a separate response for each token in isolation - they use the full accumulated context to predict the next token.\n",
    "\n",
    "## About the Final Layer\n",
    "\n",
    "You're correct about the final layer dimensions:\n",
    "- Input to the final layer: [batch_size, sequence_length, embedding_dim]\n",
    "- Output from the final layer: [batch_size, sequence_length, vocabulary_size]\n",
    "\n",
    "For each position in the sequence, the model outputs a probability distribution over the entire vocabulary. But during inference, we typically only care about the prediction for the last position in the current sequence, as that's the next token we'll generate.\n",
    "\n",
    "\n",
    "Let me clarify this point:\n",
    "\n",
    "- During training, the model calculates losses across all positions in the sequence because we're teaching it to predict each token given the previous ones.\n",
    "\n",
    "- During inference (text generation):\n",
    "  1. We process the entire sequence through the model\n",
    "  2. We only need the probability distribution for the last position\n",
    "  3. We apply softmax only to that final position's logits\n",
    "  4. We select the next token from this distribution\n",
    "  5. We append this token to the sequence\n",
    "  6. Repeat the process with the extended sequence\n",
    "\n",
    "This is more computationally efficient than computing softmax for all positions when we only need the prediction for the final position. The model architecture still processes the entire sequence through all its layers - we just don't need to convert all the outputs to probability distributions.\n",
    "\n",
    "Some implementations might still compute softmax for all positions and then only use the last one, but optimized inference engines will calculate it only for the position of interest.\n"
   ],
   "id": "e45ac4ffb5d4515c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T03:05:06.227566Z",
     "start_time": "2025-04-24T03:05:06.205952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n"
   ],
   "id": "f83efc78a1fe6784",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:58:39.936097Z",
     "start_time": "2025-04-24T04:58:39.927501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ],
   "id": "eb158d9f2749bf83",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:58:23.438477Z",
     "start_time": "2025-04-24T04:58:23.431985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.MultiHeadAttentionLayer = MultiHeadAttention(\n",
    "            d_in=config[\"emb_dim\"],\n",
    "            d_out=config[\"emb_dim\"],\n",
    "            context_length=config[\"context_length\"],\n",
    "            num_heads=config[\"n_heads\"],\n",
    "            dropout=config[\"drop_rate\"],\n",
    "            qkv_bias=config[\"qkv_bias\"])\n",
    "\n",
    "        self.ff = FeedForward(config)\n",
    "        self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
    "\n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.MultiHeadAttentionLayer(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + residual\n",
    "\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "\n",
    "        return residual + x\n",
    "\n",
    "\n"
   ],
   "id": "2201e5b727af6264",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T04:59:40.435767Z",
     "start_time": "2025-04-24T04:59:40.134969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Transformer = TransformerBlock(GPT_CONFIG_124M)\n",
    "x = torch.randn(8,4 ,768)\n",
    "\n",
    "Transformer(x)"
   ],
   "id": "12654630c6750b3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7727, -3.1875,  2.1552,  ..., -0.2826,  0.3347,  1.8124],\n",
       "         [ 1.2231, -0.3580,  0.1217,  ...,  0.3407, -0.7173, -0.4626],\n",
       "         [ 0.2187, -1.1171,  0.0045,  ..., -0.2622, -1.1334, -1.1482],\n",
       "         [-1.4615, -0.8247,  0.8350,  ...,  0.4794,  0.5317, -0.1688]],\n",
       "\n",
       "        [[ 0.2279, -0.4291, -0.5787,  ...,  1.5443,  1.0600,  0.1656],\n",
       "         [ 1.3236, -1.4566,  0.4060,  ...,  0.0772,  1.1229, -1.9444],\n",
       "         [ 0.1444,  1.3710,  0.4250,  ..., -1.1423, -1.0724,  1.0222],\n",
       "         [-0.0227, -0.4061, -0.0684,  ...,  0.7543, -0.1317,  0.2042]],\n",
       "\n",
       "        [[ 1.0829, -0.3685,  0.1287,  ..., -1.3880,  0.4187,  0.1547],\n",
       "         [ 0.3416, -1.3509,  1.0612,  ..., -0.7149,  2.2754, -0.9094],\n",
       "         [ 0.7451, -0.1145, -2.0107,  ..., -2.0818, -1.0672,  0.2608],\n",
       "         [ 0.5343,  0.6967,  0.3529,  ..., -1.0157,  1.2025,  0.0644]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.1599, -0.6663, -0.0805,  ...,  0.3496, -0.4883, -1.4553],\n",
       "         [ 0.1229, -0.5143,  2.4017,  ..., -0.7917, -1.2847, -0.6105],\n",
       "         [-0.7870, -0.6130,  1.3171,  ..., -1.4502,  0.3775, -2.0997],\n",
       "         [-1.0745, -1.5419, -0.7802,  ...,  0.0154, -1.6461,  1.7337]],\n",
       "\n",
       "        [[-1.9322,  0.2101,  0.6760,  ..., -0.0447, -0.0315,  0.5387],\n",
       "         [-0.9608, -0.8882, -0.2186,  ..., -1.3120,  1.6747, -0.7369],\n",
       "         [ 0.4137,  0.7913,  0.0321,  ...,  1.5294,  0.7731,  0.2145],\n",
       "         [-0.4047, -0.1730, -1.1542,  ...,  0.4155,  0.6833, -0.0586]],\n",
       "\n",
       "        [[ 0.7676,  0.6486,  0.4746,  ..., -0.5455,  0.8102,  1.8001],\n",
       "         [ 0.7084,  1.7806,  2.2034,  ...,  2.8199,  0.4859,  0.6429],\n",
       "         [ 0.3575, -0.0105, -1.4165,  ...,  0.8816, -0.2725,  2.0922],\n",
       "         [-0.0984,  1.2993, -1.7659,  ...,  0.6556, -0.1236,  1.0211]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T06:17:20.665887Z",
     "start_time": "2025-04-24T06:17:20.647351Z"
    }
   },
   "cell_type": "code",
   "source": "Transformer",
   "id": "7c902082653e1d5e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (MultiHeadAttentionLayer): MultiHeadAttention(\n",
       "    (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (norm1): LayerNorm()\n",
       "  (norm2): LayerNorm()\n",
       "  (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T06:47:26.649824Z",
     "start_time": "2025-04-24T06:47:26.643602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(config[\"vocab_size\"],config[\"emb_dim\"])\n",
    "        self.positional_embedding = nn.Embedding(config[\"context_length\"],config[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(config[\"drop_rate\"])\n",
    "        self.Transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
    "        )\n",
    "        self.Final_Layer_norm = LayerNorm(config[\"emb_dim\"])\n",
    "        self.output_layer = nn.Linear(\n",
    "            config[\"emb_dim\"], config[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        _,seq = x.shape\n",
    "        embedding = self.embedding_layer(x)\n",
    "        positional_encoding = self.positional_embedding(torch.arange(0,seq))\n",
    "        x = embedding + positional_encoding\n",
    "        x = self.dropout(x)\n",
    "        x = self.Transformer_blocks(x)\n",
    "        x = self.Final_Layer_norm(x)\n",
    "\n",
    "        return self.output_layer(x)\n"
   ],
   "id": "70b1362f9d742e2f",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "19a68cb037b0f189"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T06:47:29.011047Z",
     "start_time": "2025-04-24T06:47:29.000245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ],
   "id": "95c639068477c2f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T06:47:47.667567Z",
     "start_time": "2025-04-24T06:47:46.669306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Gpt = GPTModel(GPT_CONFIG_124M)\n",
    "Gpt(batch).shape"
   ],
   "id": "2873587ca66d52d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-24T07:25:43.362950Z",
     "start_time": "2025-04-24T07:25:43.353661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_num = sum(p.numel() for p in Gpt.parameters())\n",
    "total_num"
   ],
   "id": "58cd232a20f7c8af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163009536"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
